{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 14:53:12.881119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "In this environment you cannot import Basemap\n",
      "In this environment you cannot import Basemap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./xforanalogs/NA24by48/Z8/yrs500/interT15fw20.1.20lrs4/Funs.py:46: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "==Checking GPU==\n",
      "==Checking CUDA==\n",
      "==Reading data==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 14:53:16.807212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-06 14:53:16.815385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-06 14:53:17.028660: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-06 14:53:17.028709: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (apollo2048g): /proc/driver/nvidia/version does not exist\n",
      "2022-07-06 14:53:17.028725: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-06 14:53:17.030248: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "def module_from_file(module_name, file_path): #The code that imports the file which originated the training with all the instructions\n",
    "            spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            return module\n",
    "\n",
    "from importlib import import_module\n",
    "#foo = import_module(fold_folder+'/Funs.py', package=None)\n",
    "\n",
    "\n",
    "folder = './xforanalogs/NA24by48/Z8/yrs500/interT15fw20.1.20lrs4'\n",
    "foo = module_from_file(\"foo\", f'{folder}/Funs.py')\n",
    "\n",
    "import random as rd  \n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "tff = foo.tff # tensorflow routines \n",
    "ut = foo.ut # utilities\n",
    "ln = foo.ln #Learn2_new.py\n",
    "print(\"==Checking GPU==\")\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "\n",
    "print(\"==Checking CUDA==\")\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "sys.path.insert(1, '../ERA')\n",
    "\n",
    "print(\"==Reading data==\")\n",
    "\n",
    "year_permutation = np.load(f'{folder}/year_permutation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_vae_kwargs = ut.json2dict(f\"{folder}/config.json\")\n",
    "T = ut.extract_nested(run_vae_kwargs, 'T')\n",
    "if (ut.keys_exists(run_vae_kwargs, 'label_period_start') and ut.keys_exists(run_vae_kwargs, 'label_period_end')):\n",
    "    label_period_start = ut.extract_nested(run_vae_kwargs, 'label_period_start')\n",
    "    label_period_end = ut.extract_nested(run_vae_kwargs, 'label_period_end')\n",
    "    time_start = ut.extract_nested(run_vae_kwargs, 'time_start')\n",
    "    time_end = ut.extract_nested(run_vae_kwargs, 'time_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_days = time_end-time_start-T+1\n",
    "n_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 241,   0, ..., 351,   3,  52],\n",
       "       [  0,   0,  54, ..., 102, 145, 176],\n",
       "       [  0,   0, 405, ..., 390, 251, 378],\n",
       "       ...,\n",
       "       [449,  99,  99, ...,  74, 255, 123],\n",
       "       [281, 177, 241, ..., 208, 313, 198],\n",
       "       [208, 241, 211, ..., 376, 209, 339]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "fold = 0\n",
    "open_file = open(f'{folder}/fold_{fold}/analogues.pkl', \"rb\")\n",
    "analogues = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "checkpoint = 10\n",
    "\n",
    "analogues['ind_new_tr'][checkpoint]//n_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  12,   1, ...,  11,  48,  38],\n",
       "       [  1,   0,  29, ..., 100,  31,  87],\n",
       "       [  2,   3,  20, ...,  87,  93,  24],\n",
       "       ...,\n",
       "       [101,  44,  45, ...,  66,  59,  95],\n",
       "       [ 70,  46,  21, ...,  75,  24,  30],\n",
       "       [ 81,  21,  88, ...,  73,  25,  33]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogues['ind_new_tr'][checkpoint]%n_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 25317,     1, 25055, 10706, 32025, 32026, 32027,  5699],\n",
       "       [    1,     0,  5699, 42841,     2,  5701, 42840, 27301,  5700],\n",
       "       [    2,     3, 42545,  5363,  5365,  5699, 42841, 42544,     1],\n",
       "       [    3,     2,     4, 42545,  5367,  5366,  5365,  5368, 42546],\n",
       "       [    4,     5,     3,  5368,  5367, 12291,  5361,  5366, 32557],\n",
       "       [    5,     4,     6,  5368, 47154,  5361,  5367, 32557,     3],\n",
       "       [    6, 47154,     5,     7, 26782, 23223, 32562, 32561, 47155],\n",
       "       [    7,     8,  4332, 26782, 32562, 42319,  4331,     6, 47155],\n",
       "       [    8, 42320, 34685, 27246, 42014, 27245, 47156, 21432, 34236]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogues['ind_new_tr'][checkpoint][:9,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011160084656084656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   -1, 25317,    -1, ..., 36866,   363,  5498],\n",
       "       [   -1,    -1,  5699, ..., 10810, 15256, 18567],\n",
       "       [   -1,    -1, 42545, ..., 41037, 26448, 39714],\n",
       "       ...,\n",
       "       [   -1, 10439, 10440, ...,  7836, 26834, 13010],\n",
       "       [29575, 18631, 25326, ..., 21915, 32889, 20820],\n",
       "       [21921, 25326, 22243, ..., 39553, 21970, 35628]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#selfanalogues = (analogues['ind_new_tr'][checkpoint]//n_days)[:,0][np.newaxis].T\n",
    "selfanalogues = ((np.arange(analogues['ind_new_tr'][checkpoint].shape[0]//n_days)[np.newaxis].T*np.ones((analogues['ind_new_tr'][checkpoint].shape[0]//n_days,n_days))).astype(int)).flatten()[np.newaxis].T # This is a matrix of years related to the raw index\n",
    "#selfanalogues*np.ones((selfanalogues.shape[0],analogues['ind_new_tr'][checkpoint].shape[1]))\n",
    "sameyear = selfanalogues*np.ones((selfanalogues.shape[0],analogues['ind_new_tr'][checkpoint].shape[1]))==analogues['ind_new_tr'][checkpoint]//n_days # A conditional matrix showing if the entry belongs to the same year \n",
    "#sameyear\n",
    "noselfanalogs = (np.where(sameyear,-1,analogues['ind_new_tr'][checkpoint])) # We set to -1 all the entries that are analogs of the same year\n",
    "print(np.mean(noselfanalogs == -1))\n",
    "noselfanalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25317, 25055, 10706, ..., 47250, 47250, 47250],\n",
       "       [ 5699, 42841,  5701, ..., 47250, 47250, 47250],\n",
       "       [42545,  5363,  5365, ..., 47250, 47250, 47250],\n",
       "       ...,\n",
       "       [10439, 10440, 26960, ..., 47250, 47250, 47250],\n",
       "       [29575, 18631, 25326, ..., 20820, 47250, 47250],\n",
       "       [21921, 25326, 22243, ..., 39553, 21970, 35628]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noselfanalogsmoved = []\n",
    "for noselfanalogs_row in noselfanalogs: # loop over samples\n",
    "    temp = np.delete(noselfanalogs_row,noselfanalogs_row==-1) # remove the values equal to -1 \n",
    "    noselfanalogsmoved.append(np.pad(temp, (0,noselfanalogs_row.shape[0] - temp.shape[0]), constant_values=(noselfanalogs.shape[0],noselfanalogs.shape[0])))  # pad with the length of the time series so that if we accidently get such analog the error will be returned\n",
    "noselfanalogsmoved = np.array(noselfanalogsmoved)\n",
    "noselfanalogsmoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogues['ind_new_va'][checkpoint].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_tr = np.load('./xforanalogs/NA24by48/Z8/yrs500/interT15fw20.1.20lrs4/fold_0/time_series_tr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47250,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_tr[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "from numba import jit,guvectorize,set_num_threads\n",
    "\n",
    "num_Traj = 10000 #number of trajectories for monte-carlo sampling of committor\n",
    "Matr_tr = noselfanalogsmoved\n",
    "Matr_va = analogues['ind_new_va'][checkpoint]\n",
    "delay = np.array([0,3,6,9,12,15])  #time prediction (what we call lead time tau)\n",
    "Temp = time_series_tr[:,0]\n",
    "neighbors = [10]\n",
    "\n",
    "thershold = np.array([2.742728328704834]) #Threshold defining committor. This parameter I don't need, I shall perhaps transform it into epochs for variational autoencoder\n",
    "\n",
    "step_Traj = 5  #number of steps in the markov chain for 15 days.\n",
    "#Function for computing the committor at one point. Input: day=point where committor is computed (state of markov chain), ther= vector of threshold, dela= vector of delays, nn= number of neighbors, Matr=Matix which contains indeces of Markov chain, res= vector where results are stored\n",
    "@guvectorize([(nb.int64,nb.float64[:],nb.int64[:],nb.int64,nb.int64,nb.int64[:,:],nb.int64[:,:],nb.float64[:,:])],'(),(n),(m),(),(),(k,o),(l,j)->(n,m)',nopython=True,target=\"parallel\")\n",
    "def CommOnePoint(day,ther,dela,nn, n_Traj, Matr_va, Matr_tr,res): # a day implies the temporal coordinates in days of the input from the 0-th year of the 1000 year long dataset\n",
    "    if day>Matr_va.shape[0]:\n",
    "        #print(\"day > Matr_va.shape[0]\")\n",
    "        for l_1 in range(len(ther)):\n",
    "            for l_2 in range(len(dela)):\n",
    "                res[l_1][l_2] = np.nan # If day is larger we simply  don't have corresponding index\n",
    "    else:\n",
    "        #print(\"day <= Matr_va.shape[0]\")\n",
    "        z = np.zeros((len(ther),len(dela))) #auxiliary variable (result)\n",
    "        key_day = 0     #parameter for initial conditions\n",
    "        for i in range(n_Traj):\n",
    "            if(key_day>0): # if we are in the train set\n",
    "                s_0 = day\n",
    "            else: # if we are in the validation set\n",
    "                app = rd.randint(0,nn-1) # we go randomly to the training dataset from the validation dataset without updating the time\n",
    "                s_0 = Matr_va[day][app]\n",
    "                #print(\"output: \", day,app,s_0, Matr_va.shape)\n",
    "            \n",
    "            A = np.zeros((len(ther),len(dela))) #auxiliary variable (integrated temperature)\n",
    "            \n",
    "            s = s_0 # we start here \n",
    "            for j in range(step_Traj+np.max(dela)//3): # iterate so that we have a history before tau = 0 if tau is less than 0. It seems a bit strange we are considering 5 consecutive steps plus tau. Maybe your definition of tau is changed by 3 days?\n",
    "                app = rd.randint(0,nn-1) #analogue selection\n",
    "                s = Matr_tr[s][app] + 3         #analog state s is evolved in time\n",
    "                for l_2 in range(len(dela)): # It is confusing that there is a reference to both dela and delay, where delay is generally an input of this function but in principle can serve as a global variable\n",
    "                    if(j>=dela[l_2]//3 and j<dela[l_2]//3+step_Traj):\n",
    "                        for l_1 in range(len(ther)):\n",
    "                            A[l_1][l_2] += Temp[s] ## GM: we start counting A only when we get into this delay window\n",
    "            A = A / 5. \n",
    "            \n",
    "            #Check if A>a\n",
    "            for l_1 in range(len(ther)):\n",
    "                for l_2 in range(len(dela)):\n",
    "                    if(A[l_1][l_2]>ther[l_1]):\n",
    "                        z[l_1][l_2] += 1.\n",
    "        #fill res vector\n",
    "        for l_1 in range(len(ther)):\n",
    "            for l_2 in range(len(dela)):\n",
    "                res[l_1][l_2] = z[l_1][l_2] / n_Traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1998 0.141  0.1007 0.0717 0.0564 0.0516]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = neighbors[0]\n",
    "q_1 = CommOnePoint(5250,thershold,delay,nn,10000,Matr_va,Matr_tr)\n",
    "#print(y*90+89)\n",
    "print(q_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1202 0.0716 0.0447 0.0404 0.0375 0.0403]]\n",
      "\n",
      " [[0.0529 0.0368 0.0345 0.0378 0.0396 0.0416]]\n",
      "\n",
      " [[0.0505 0.0369 0.0379 0.0389 0.0389 0.0388]]\n",
      "\n",
      " [[0.0604 0.0402 0.038  0.0389 0.0414 0.0432]]]\n",
      "[[[0.1202 0.0716 0.0447 0.0404 0.0375 0.0403]\n",
      "  [0.0529 0.0368 0.0345 0.0378 0.0396 0.0416]]\n",
      "\n",
      " [[0.0505 0.0369 0.0379 0.0389 0.0389 0.0388]\n",
      "  [0.0604 0.0402 0.038  0.0389 0.0414 0.0432]]]\n"
     ]
    }
   ],
   "source": [
    "days =  np.arange(4)\n",
    "q_1 = CommOnePoint(days,thershold,delay,nn,10000,Matr_va,Matr_tr)\n",
    "#print(y*90+89)\n",
    "print(q_1)\n",
    "print(q_1.reshape(2,2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5250, 1, 6)\n",
      "0.1148\n",
      "(50, 105, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "days =  np.arange(Matr_va.shape[0])\n",
    "q = CommOnePoint(days,thershold,delay,nn,10000,Matr_va,Matr_tr)\n",
    "#print(y*90+89)\n",
    "print(q.shape)\n",
    "print(q[0,0,0])\n",
    "print(q.reshape(-1,n_days,len(delay)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days = array([5249, 5250, 5251, 5252])\n",
      "[[[0.  0.2 0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.2 0.2 0.2 0.2 0.2 0.2]]\n",
      "\n",
      " [[nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan]]]\n"
     ]
    }
   ],
   "source": [
    "#days = np.array(range(y*n_days,(y+100)*n_days))\n",
    "days =  np.array(range(Matr_va.shape[0]-1,Matr_va.shape[0]+3))\n",
    "print(f\"{days = }\")\n",
    "q = CommOnePoint(days,thershold,delay,nn,Matr_va,Matr_tr)\n",
    "#print(y*90+89)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_va = np.load(f\"{folder}/fold_{fold}/Y_va.npy\")\n",
    "Y_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#days = np.array(range(y*n_days,(y+100)*n_days))\n",
    "#print(f\"{days = }\")\n",
    "#q = CommOnePoint(days,thershold,delay,nn,Matr_va,Matr_tr)\n",
    "#print(q.shape)\n",
    "#print(q[len(q)-1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 10497, 10498, 10499])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matr_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "13d6ac2b46cffe7b04dd84f7288304bd0f3c8f660cff0949df6ccaecc97804f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
